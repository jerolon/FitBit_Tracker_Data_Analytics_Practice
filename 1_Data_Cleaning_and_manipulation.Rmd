---
title: "Data Cleaning and manipulation"
author: "Jeronimo Miranda"
date: '2023-05-16'
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(skimr)
library(dplyr)
library(lubridate)
library(stringr)
library(tidyr)
library(janitor)
library(ggplot2)
```

```{r directory setup, include = FALSE}
rstudioapi::getActiveDocumentContext
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

## Data Loading

I will load the corresponding files from both folders and merge them using rbind. I will only show the code for loading and merging for dailyActivity, because it is repetitive, but will show the summaries and cleaning for all. In any case, the code is still in the .Rmd file. 

```{r data loading, message=FALSE, warning=FALSE}
data_path_A <- "../Fitabase Data 3.12.16-4.11.16/"
data_path_B <- "../Fitabase Data 4.12.16-5.12.16/"
dailyActivity_A <- read_csv(paste0(data_path_A,"dailyActivity_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityDate = col_date(format = "%m/%d/%Y")))

dailyActivity_B <- read_csv(paste0(data_path_B,"dailyActivity_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityDate = col_date(format = "%m/%d/%Y")))

```
We also have a problem at the intersection of the two date ranges. There are duplicate entries at 2016-04-12 for many Ids. We will remove this date in the first object because the data is supposed to go only to 2016-04-11.
Aditionally, will remove the intermediate objects, once merged (the rm() function).

```{r merge}
dailyActivity_A <- dailyActivity_A %>% filter(ActivityDate < ymd(20160412))
dailyActivity <- rbind(dailyActivity_A, dailyActivity_B)
rm(dailyActivity_A)
rm(dailyActivity_B)
skim_without_charts(dailyActivity)
```

We must note the inconsistency between the name of the columns ModeratelyActiveDistance and FairlyActiveMinutes, which correspond to the same intensity category.

### Daily sleep

Interestingly, there is no daily sleep data for the 3.11 - 4.11 period.

```{r sleep day}
sleepDay <- read_csv(paste0(data_path_B,"sleepDay_merged.csv"), 
    col_types = cols(Id = col_character(), 
        SleepDay = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))
skim_without_charts(sleepDay)
```

### Weight info

Weight info is absent in the 3.11 - 4.11 period too
```{r weight info, message=FALSE, warning=FALSE, echo=FALSE}
weightLogInfo <- read_csv(paste0(data_path_B,"weightLogInfo_merged.csv"),
    col_types = cols(Id = col_character(), 
        Date = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))
```

```{r weight info show, message=FALSE, warning=FALSE}
skim_without_charts(weightLogInfo)
```
### Hourly data

```{r hourly loading, warning=FALSE, echo=FALSE}
hourlyCalories_A <- read_csv(paste0(data_path_A, "hourlyCalories_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlyCalories_B <- read_csv(paste0(data_path_B, "hourlyCalories_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlyCalories <- rbind(hourlyCalories_A, hourlyCalories_B)
rm(hourlyCalories_A); rm(hourlyCalories_B)

hourlyIntensities_A <- read_csv(paste0(data_path_A, "hourlyIntensities_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlyIntensities_B <- read_csv(paste0(data_path_B, "hourlyIntensities_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlyIntensities <- rbind(hourlyIntensities_A, hourlyIntensities_B)
rm(hourlyIntensities_A); rm(hourlyIntensities_B)

hourlyStepsA <- read_csv(paste0(data_path_A, "hourlySteps_merged.csv"), col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlyStepsB <- read_csv(paste0(data_path_B, "hourlySteps_merged.csv"), col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlySteps <- rbind(hourlyStepsA, hourlyStepsB)
rm(hourlyStepsA); rm(hourlyStepsB)

hourlyActivity <- inner_join(hourlyCalories, hourlyIntensities, by = c("Id","ActivityHour")) %>% inner_join(hourlySteps, by = c("Id", "ActivityHour"))

hourlyActivity <- distinct(hourlyActivity)
rm(hourlyCalories, hourlyIntensities, hourlySteps)
```

```{r hourly loading show, warning=FALSE}
skim_without_charts(hourlyActivity)
```

### Minute data

I will only check the narrow minute files.

```{r minute data loading, message=FALSE, warning=FALSE, include=FALSE}
minuteCaloriesA <- read_csv(paste0(data_path_A, "minuteCaloriesNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

minuteCaloriesB <- read_csv(paste0(data_path_B, "minuteCaloriesNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))


minuteIntensitiesA <- read_csv(paste0(data_path_A,"minuteIntensitiesNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))
minuteIntensitiesB <- read_csv(paste0(data_path_B,"minuteIntensitiesNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

minuteStepsA <- read_csv(paste0(data_path_A,"minuteStepsNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

minuteStepsB <- read_csv(paste0(data_path_B,"minuteStepsNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

minuteCalories <- rbind(minuteCaloriesA, minuteCaloriesB)
rm(minuteCaloriesA, minuteCaloriesB)

minuteSteps <- rbind(minuteStepsA, minuteStepsB)
rm(minuteStepsA, minuteStepsB)

minuteIntensities <- rbind(minuteIntensitiesA, minuteIntensitiesB)
rm(minuteIntensitiesA, minuteIntensitiesB)
```

```{r minute skim, message=FALSE, warning=FALSE}
minuteActivity <- inner_join(minuteCalories, minuteIntensities, by = c("Id","ActivityMinute")) %>% inner_join(minuteSteps, by = c("Id", "ActivityMinute"))
skim_without_charts(minuteActivity)
```

### Heart rate data

```{r load heart rate, warning=FALSE, echo=FALSE}
heartrate_seconds_A <- read_csv(paste0(data_path_A,"heartrate_seconds_merged.csv"), col_types = cols(Id = col_character(), Time = col_datetime(format = "%m/%d/%Y %H:%M:%S %p"), Value = col_double()))

heartrate_seconds_B <- read_csv(paste0(data_path_B,"heartrate_seconds_merged.csv"), col_types = cols(Id = col_character(), Time = col_datetime(format = "%m/%d/%Y %H:%M:%S %p"), Value = col_double()))
## Splitting the date time column into date and time
heartrate_seconds <- rbind(heartrate_seconds_A, heartrate_seconds_B)
rm(heartrate_seconds_A)
rm(heartrate_seconds_B)
```

```{r show heart rate, warning=FALSE}
skim_without_charts(heartrate_seconds)
```
## Data validation and cleaning

#### Checking Ids

Checking the he skim without charts tables shows that no Id data is missing for any row, and that all have 10 characters. Next, I want to make sure that there are no misspelings such that an Id in a dataset has no correspondance to another. First, we will take the 35 unique Ids in the `dailyActivity` table as our reference. Then, we will use set union or set equality to check that there are no extra Ids in the other datasets.

```{r Id check}
uniqueIds <- unique(dailyActivity$Id)
### Datasets with the full 35 participants
#Hour data
setequal(uniqueIds, unique(hourlyActivity$Id))
#Minute data
setequal(uniqueIds, unique(minuteActivity$Id))

### Datasets with some participants missing
#heart rate
union(uniqueIds, unique(heartrate_seconds$Id)) %>% length
#weight
union(uniqueIds, unique(weightLogInfo$Id)) %>% length
#sleep
union(uniqueIds, unique(sleepDay$Id)) %>% length
```
TRUE for set equal means the set of Ids are identical. For the incomplete data sets heart rate, weight and sleep, we know some Ids are missing. In these cases we do set union, which would give a list longer than 35 if the Ids from those data sets were not completely included in the reference.

### Checking dates
We know that dates are correct because of how we imported them with readr. If any data had not been recognized, they would have shown as missing values in the skim without charts tables. What I am interested is in the range of dates we have. We will use the geom tile as a nice way to plot each day with its calorie expenditure value. 

```{r calendar plot of activity, echo = FALSE}
ggplot(dailyActivity, aes(x = ActivityDate, y = Id)) + geom_tile(aes(fill = Calories)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Daily Calorie expenditure", subtitle = "Daily calorie expenditure by user throught the study period")
```

I love this graph because it immediately tells a lot of info: why it was decided to cut the original kaggle data set from april 12, which users have gaps, and suspicious days of very low calorie expenditure. It is a sort of conditional formatting for the data. 
I prefer to keep all available dates for now, because there is no need for the data to be exactly simultaneous.

Nevertheless, I will cut out the 9 days of **zero** calorie expenditure, since that is physically very unlikely and indicates an artifact. We also check there is only one record per date per Id. Our final sanity check for this data set is to sum the total number of minutes of all activity types, and check that it is not greater than 1440, which is the total number of minutes in a date.

```{r filter daily zero cal}
dailyActivity <- dailyActivity %>% filter(Calories > 1)

#Check that there are no Id, Date duplicate records
get_dupes(dailyActivity, Id, ActivityDate)

#Check that the sum of minutes does not exceed the total minutes in a day
dailyActivity %>% mutate(totalMinutes = VeryActiveMinutes + FairlyActiveMinutes + LightlyActiveMinutes + SedentaryMinutes) %>% filter(totalMinutes > 24 * 60)
```

#### Sleep data
Sleep data is very irregular and it is not clear why. There is also a tendency for the users with very few days to have too little or too much sleep. I will not filter out these users because there is also information of how they use the device, so it is useful. It has to be kept in mind when analyzing the sleep durations, though. The number of minutes sleeping in general seems plausible.

```{r plotting sleep data, echo=FALSE}
ggplot(sleepDay, aes(x = SleepDay, y = Id)) + geom_tile(aes(fill = TotalMinutesAsleep)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Daily Sleep", subtitle = "Daily minutes spent asleep by user")
```

The sanity checks are no record Id/SleepDay duplication and that minutes in a day do not exceed $60 * 24 = 1440$, although we have already seen this is not the case in the skim_without_charts summary.
```{r clean sleep}
get_dupes(sleepDay, Id, SleepDay)
##There are duplicate rows
sleepDay <- distinct(sleepDay)

sleepDay %>% filter(TotalMinutesAsleep >= 1440, TotalTimeInBed >= 1440)
```


#### Checking the date time in the hourly data

Checking for duplicates of Id/hour combination. Interestingly, there are no hours with zero calories, which is perplexing given that I thought the daily activity file was just a summary from this one.

```{r}
hourlyActivity %>% get_dupes(Id, ActivityHour)
#zero calory hours
filter(hourlyActivity, Calories == 0)
# days with more than 24 hours. Not really needed, only possible if get_dupes had returned something
hourlyActivity %>% mutate(Day = date(ActivityHour)) %>% group_by(Id, Day) %>% summarise(number_of_hours = n()) %>% filter(number_of_hours > 24)
```

We will not graph all the hours, instead we will group by day.

```{r echo=FALSE, message=FALSE, warning=FALSE}
hourly2dailyActivity <- hourlyActivity %>% mutate(Day = date(ActivityHour)) %>% group_by(Id, Day) %>% summarise(number_of_hours = n(), DCalories = sum(Calories), DStep = sum(StepTotal)) 

hourly2dailyActivity %>% ggplot(aes(x = Day, y = Id)) + geom_tile(aes(fill = DCalories)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Daily Calorie expenditure", subtitle = "Hourly calorie expenditure by user summarised by date")
```

This is really baffling. There is Calories data for almost all users for the first half of the study, the data is collected for 24 hours except on the last days. This data is missing in the DailyActivity (compare with the previous graphs). This might be due to the lack of Distance data for the relevant dates. Nevertheless, it could be useful to use this aggregated data if we only want to look at Calories or steps. Lastly, we might consider getting rid of Id "2891001357" with too few data points, but we will wait until we check the minute-level data.

Next, we will compare this aggregated data set with the existing dailyActivity data frame. We use a left join to keep only the dates that exist in the original dailyActivity.

```{r, echo=FALSE}
left_join(dailyActivity, hourly2dailyActivity, by = join_by(Id, ActivityDate == Day)) %>% transmute(Id, ActivityDate, diffCal = Calories - DCalories, diffSteps = TotalSteps - DStep, number_of_hours) %>% ggplot(aes(x = ActivityDate, y = Id)) + geom_tile(aes(fill = diffCal)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Daily Calorie error", subtitle = "Difference between daily and hourly aggregated data per user per day")
```

Most days coincide, some error could be expected due to aggregation issues or rounding. But what about the substantial divergences? The biggest discrepancies occur in days that are not on the edges and that have the full 24 hours. One of these days is also the maximum calories in the activity data. The other is an instance in which the number of sedentary minutes is 1440 (The whole day), yet the distance and number of steps are huge. I do not see how this is possible, only if the person kept the tracker in the backpack all day or something, therefore, I will erase these two records that differ the most in steps or calories between the daily and the hourly data.

```{r echo=FALSE}
left_join(dailyActivity, hourly2dailyActivity, by = join_by(Id, ActivityDate == Day)) %>% mutate(diffCal = Calories - DCalories, diffSteps = TotalSteps - DStep) %>% filter(abs(diffCal) == max(abs(diffCal), na.rm = T))

left_join(dailyActivity, hourly2dailyActivity, by = join_by(Id, ActivityDate == Day)) %>% mutate(diffCal = Calories - DCalories, diffSteps = TotalSteps - DStep) %>% filter(abs(diffSteps) == max(abs(diffSteps), na.rm = T))

dailyActivity <- dailyActivity %>%  filter(!(Id == "8583815059" & ActivityDate == ymd(20160503)))

dailyActivity <- dailyActivity %>%  filter(!(Id == "6117666160" & ActivityDate == ymd(20160421)))
```

### Daily activity revisited

This motivates me to explore the relationships in the dailyActivity data to understand it better. It seems from the graph that The "TotalDistance" is calculated from the number of steps for each individual user. That is the reason that TotalDistance is different from tracked distance, which comes from GPS.

```{r steps and distance, echo = F}
dailyActivity %>% ggplot() + geom_point(aes(x = TotalSteps, y = TotalDistance)) + theme_bw() + labs(title = "The Total distance is calculated from Steps", subtitle = "Daily Total steps have a linear relationship with distance")
```

We can sum the active distances from different excercise intensities (ACtive, moderate, Light and Sendentary) and check their relationship with the total distance.

```{r}
dailyActivity %>% mutate(ActiveDistances = VeryActiveDistance + ModeratelyActiveDistance + LightActiveDistance + SedentaryActiveDistance)  %>%  ggplot() + geom_point(aes(x = ActiveDistances, y = TotalDistance)) + theme_bw() + labs(title = "Sum of active distances vs Total Distance", subtitle = "The distances roughly coincide")
```

We see that the graph is completely linear and that the sum of the distances is the same as the total distance in most cases. The total distance is never less than the sum of Active Distances, showing that the error is not random. I do not trust the points that diverge by more than 1 km. Specially the ones where the Active Distance is zero. An explanation for this is that people carried their device that day on their backpack or something, therefore steps or GPS were recorded, but not the heartrate, which determines intensity. Therefore, we will filter them out:

```{r filter divergent distances}
dailyActivity <- dailyActivity %>% mutate(ActiveDistances = VeryActiveDistance + ModeratelyActiveDistance + LightActiveDistance + SedentaryActiveDistance, differencia = TotalDistance - ActiveDistances)  %>%  filter(differencia < 1) %>% select(-ActiveDistances)
```
I do not show this but this filter has the advantage of dropping more of the highly divergent data from the hourly data we saw above (see graph Daily Calorie Error), mostly for the user "402033". This gives me more confidence that dropping these data is correct.

#### Velocities

I want to do a further sanity check on the Active columns. I want to check that there are no instances where VeryActiveDistance of 0 corresponds to a non-zero VeryActiveMinutes, which would be inconsistent. For this, we can use the xor function. Xor() will return FALSE if the data is correct, that is if both values are non-zero, or if both are zero, any discrepancy will result in a TRUE value. Of course, it is possible to have 0 distance with positive minutes, by working out in a stationary bike, for example, so we further filter to get only the rows where the time is zero but the distance is non-zero.

```{r}
filter_vector <- xor(dailyActivity$VeryActiveDistance, dailyActivity$VeryActiveMinutes)
filter(dailyActivity, filter_vector, VeryActiveMinutes == 0)

filter_vector <- xor(dailyActivity$ ModeratelyActiveDistance, dailyActivity$ FairlyActiveMinutes)
filter(dailyActivity, filter_vector, FairlyActiveMinutes == 0)

filter_vector <- xor(dailyActivity$LightActiveDistance, dailyActivity$LightlyActiveMinutes)
filter(dailyActivity, filter_vector, LightlyActiveMinutes == 0)

filter_vector <- xor(dailyActivity$SedentaryActiveDistance, dailyActivity$SedentaryMinutes)
filter(dailyActivity, filter_vector, SedentaryMinutes == 0)
```

Our data is consistent, at least in this way. Being sure of this, now I want to know the average velocity of people in the different categories. Multiplication by 60 is done to transform minutes to km/hour

```{r velocities, echo=FALSE, warning=FALSE}
dailyActivity %>% mutate(HighVelocity = 60 * VeryActiveDistance/VeryActiveMinutes,  ModerateVelocity = 60 * ModeratelyActiveDistance/FairlyActiveMinutes, LightVelocity = 60 *  LightActiveDistance/LightlyActiveMinutes, SedentaryVelocity = 60 * SedentaryActiveDistance/SedentaryMinutes) %>% 
    #it has to be transformed to long format
    pivot_longer(names_to = "Intensity_Class", values_to = "Velocity", cols = ends_with("Velocity")) %>% 
    #Boxplot
    ggplot(aes(x = Intensity_Class, y = Velocity)) + geom_jitter(alpha = 0.2) + geom_boxplot(outlier.shape = NA) + theme_bw() 
```

This is at the same time a sanity check (no one is travelling at 100 km/h, no sedentary velocity is significantly above 0) and an overview of the kind of activities people are doing. A few jog. Most walk (or swim?). The zero velocities in all categories apart from sedentary are probably people on a treadmill or stationary bike, or lifting weights. Lower than average velocities could be people hiking, swimming or going upstairs: intense activities that do not cover much distance.


## Plotting a few explorations

What's the relationship between steps taken in a day and sedentary minutes? How could this help inform the customer segments that we can market to? E.g. position this more as a way to get started in walking more? Or to measure steps that you're already taking?

```{r}
ggplot(data=dailyActivity, aes(x=TotalSteps, y=SedentaryMinutes)) + geom_point()
```

What's the relationship between minutes asleep and time in bed? You might expect it to be almost completely linear - are there any unexpected trends?

```{r}
ggplot(data=sleepDay, aes(x=TotalMinutesAsleep, y=TotalTimeInBed)) + geom_point()
```

What could these trends tell you about how to help market this product? Or areas where you might want to explore further?

Note that there were more participant Ids in the daily activity dataset that have been filtered out using merge. Consider using 'outer_join' to keep those in the dataset. 

Now you can explore some different relationships between activity and sleep as well. For example, do you think participants who sleep more also take more steps or fewer steps per day? Is there a relationship at all? How could these answers help inform the marketing strategy of how you position this new product?

This is just one example of how to get started with this data - there are many other files and questions to explore as well!

