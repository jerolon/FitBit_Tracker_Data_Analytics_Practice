---
title: "Data Cleaning and manipulation"
author: "Jeronimo Miranda"
date: '2023-05-16'
output: github_document
---

```{r 1_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(skimr)
library(dplyr)
library(lubridate)
library(stringr)
library(tidyr)
library(janitor)
library(ggplot2)
```

```{r directory_setup, include = FALSE}
rstudioapi::getActiveDocumentContext
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
```

## Data Loading

I will load the corresponding files from both folders and merge them using rbind. I will only show the code for loading and merging for dailyActivity, because it is repetitive, but will show the summaries and cleaning for all. In any case, the code is still in the .Rmd file. 

```{r data loading, message=FALSE, warning=FALSE}
data_path_A <- "../Fitabase Data 3.12.16-4.11.16/"
data_path_B <- "../Fitabase Data 4.12.16-5.12.16/"
dailyActivity_A <- read_csv(paste0(data_path_A,"dailyActivity_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityDate = col_date(format = "%m/%d/%Y")))

dailyActivity_B <- read_csv(paste0(data_path_B,"dailyActivity_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityDate = col_date(format = "%m/%d/%Y")))

```
We also have a problem at the intersection of the two date ranges. There are duplicate entries at 2016-04-12 for many Ids. We will remove this date in the first object because the data is supposed to go only to 2016-04-11.
Aditionally, will remove the intermediate objects, once merged (the rm() function).

```{r merge, warning=FALSE}
dailyActivity_A <- dailyActivity_A %>% filter(ActivityDate < ymd(20160412))
dailyActivity <- rbind(dailyActivity_A, dailyActivity_B)
rm(dailyActivity_A)
rm(dailyActivity_B)
skim_without_charts(dailyActivity)
```

When comparing different kinds of activities, we must take into account the inconsistency between the name of the columns ModeratelyActiveDistance and FairlyActiveMinutes, which correspond to the same intensity category.

### Daily sleep

Interestingly, there is no daily sleep data for the 3.11 - 4.11 period.

```{r sleep day}
sleepDay <- read_csv(paste0(data_path_B,"sleepDay_merged.csv"), 
    col_types = cols(Id = col_character(), 
        SleepDay = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))
skim_without_charts(sleepDay)
```

### Weight info

Weight info is absent in the 3.11 - 4.11 period too
```{r weight info, message=FALSE, warning=FALSE, echo=FALSE}
weightLogInfo <- read_csv(paste0(data_path_B,"weightLogInfo_merged.csv"),
    col_types = cols(Id = col_character(), 
        Date = col_datetime(format = "%m/%d/%Y %H:%M:%S %p"),
        LogId = col_character()))
```

```{r weight info show, message=FALSE, warning=FALSE}
skim_without_charts(weightLogInfo)
```

Only 8 distinct users have any weight info. There are only two entries for bodyfat. All weight data are plausible, no one is 0kg or negative weight. The conversion factor is 2.204623 Pound per Kg and it is correct for all. All the logIDs are not unique, though. It seems the LogId is a function of the date and time. When a manual entry is recorded, the tracker enters just the date and when two users enter a manual record at the same time, they get the exact same LogId. It is not a problem because there are no (LogId, Id) duplicates.

```{r weight cleaning}
get_dupes(weightLogInfo)

weightLogInfo %>% group_by(Id) %>% summarise(sd_kg = sd(WeightKg))
```

The estandard deviations can only be calculated for whomever entered 3 records or more. No one had wild fluctuations that indicated an error.

### Hourly data

```{r hourly loading, warning=FALSE, echo=FALSE}
hourlyCalories_A <- read_csv(paste0(data_path_A, "hourlyCalories_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlyCalories_B <- read_csv(paste0(data_path_B, "hourlyCalories_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlyCalories <- rbind(hourlyCalories_A, hourlyCalories_B)
rm(hourlyCalories_A); rm(hourlyCalories_B)

hourlyIntensities_A <- read_csv(paste0(data_path_A, "hourlyIntensities_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlyIntensities_B <- read_csv(paste0(data_path_B, "hourlyIntensities_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlyIntensities <- rbind(hourlyIntensities_A, hourlyIntensities_B)
rm(hourlyIntensities_A); rm(hourlyIntensities_B)

hourlyStepsA <- read_csv(paste0(data_path_A, "hourlySteps_merged.csv"), col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlyStepsB <- read_csv(paste0(data_path_B, "hourlySteps_merged.csv"), col_types = cols(Id = col_character(), 
        ActivityHour = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

hourlySteps <- rbind(hourlyStepsA, hourlyStepsB)
rm(hourlyStepsA); rm(hourlyStepsB)

hourlyActivity <- inner_join(hourlyCalories, hourlyIntensities, by = c("Id","ActivityHour")) %>% inner_join(hourlySteps, by = c("Id", "ActivityHour"))

hourlyActivity <- distinct(hourlyActivity)
rm(hourlyCalories, hourlyIntensities, hourlySteps)
```

```{r hourly loading show, warning=FALSE}
skim_without_charts(hourlyActivity)
```

### Minute data

I will only check the narrow minute files.

```{r minute data loading, message=FALSE, warning=FALSE, include=FALSE}
minuteCaloriesA <- read_csv(paste0(data_path_A, "minuteCaloriesNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

minuteCaloriesB <- read_csv(paste0(data_path_B, "minuteCaloriesNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))


minuteIntensitiesA <- read_csv(paste0(data_path_A,"minuteIntensitiesNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))
minuteIntensitiesB <- read_csv(paste0(data_path_B,"minuteIntensitiesNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

minuteStepsA <- read_csv(paste0(data_path_A,"minuteStepsNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

minuteStepsB <- read_csv(paste0(data_path_B,"minuteStepsNarrow_merged.csv"), 
    col_types = cols(Id = col_character(), 
        ActivityMinute = col_datetime(format = "%m/%d/%Y %H:%M:%S %p")))

minuteCalories <- rbind(minuteCaloriesA, minuteCaloriesB)
rm(minuteCaloriesA, minuteCaloriesB)

minuteSteps <- rbind(minuteStepsA, minuteStepsB)
rm(minuteStepsA, minuteStepsB)

minuteIntensities <- rbind(minuteIntensitiesA, minuteIntensitiesB)
rm(minuteIntensitiesA, minuteIntensitiesB)
```

```{r minute skim, message=FALSE, warning=FALSE}
minuteActivity <- inner_join(minuteCalories, minuteIntensities, by = c("Id","ActivityMinute")) %>% inner_join(minuteSteps, by = c("Id", "ActivityMinute"))
minuteActivity <- distinct(minuteActivity)
skim_without_charts(minuteActivity)
```

### Heart rate data

```{r load heart rate, warning=FALSE, echo=FALSE}
heartrate_seconds_A <- read_csv(paste0(data_path_A,"heartrate_seconds_merged.csv"), col_types = cols(Id = col_character(), Time = col_datetime(format = "%m/%d/%Y %H:%M:%S %p"), Value = col_double()))

heartrate_seconds_B <- read_csv(paste0(data_path_B,"heartrate_seconds_merged.csv"), col_types = cols(Id = col_character(), Time = col_datetime(format = "%m/%d/%Y %H:%M:%S %p"), Value = col_double()))
## Splitting the date time column into date and time
heartrate_seconds <- rbind(heartrate_seconds_A, heartrate_seconds_B)
heartrate_seconds <- distinct(heartrate_seconds)
rm(heartrate_seconds_A)
rm(heartrate_seconds_B)
```

```{r show heart rate, warning=FALSE}
skim_without_charts(heartrate_seconds)
```

Heart rates are quite plausible, the minimum is 36, so no one is dead. The maximum is 200, which is high, but it is alright. The median is 74, which is a normal heart beat.

## Data validation and cleaning

#### Checking Ids

Checking the skim without charts tables shows that no Id data is missing for any row, and that all have 10 characters. Next, I want to make sure that there are no misspelings such that an Id in a dataset has no correspondance to another. First, we will take the 35 unique Ids in the `dailyActivity` table as our reference. Then, we will use set union or set equality to check that there are no extra Ids in the other datasets.

```{r Id check}
uniqueIds <- unique(dailyActivity$Id)
### Datasets with the full 35 participants
#Hour data
setequal(uniqueIds, unique(hourlyActivity$Id))
#Minute data
setequal(uniqueIds, unique(minuteActivity$Id))

### Datasets with some participants missing
#heart rate
union(uniqueIds, unique(heartrate_seconds$Id)) %>% length
#weight
union(uniqueIds, unique(weightLogInfo$Id)) %>% length
#sleep
union(uniqueIds, unique(sleepDay$Id)) %>% length
```
TRUE for set equal means the set of Ids are identical. For the incomplete data sets heart rate, weight and sleep, we know some Ids are missing. In these cases we do set union, which would give a list longer than 35 if the Ids from those data sets were not completely included in the reference.

### Checking dates and cleaning
We know that dates are correct because of how we imported them with *readr*. If any data had not been recognized, they would have shown as missing values in the skim without charts tables. What I am interested is in the range of dates we have. We will use ggplot2::geom_tile as a nice way to plot each day with its calorie expenditure value. 

```{r calendar plot of activity, echo = FALSE}
ggplot(dailyActivity, aes(x = ActivityDate, y = Id)) + geom_tile(aes(fill = Calories)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Daily Calorie expenditure", subtitle = "Daily calorie expenditure by user throught the study period")
```

I love this graph because it immediately tells a lot of info: why it was decided to cut the original kaggle data set from april 12, which users have gaps, and suspicious days of very low calorie expenditure. It is a sort of conditional formatting for the data. 
I prefer to keep all available dates for now, because there is no need for the data to be exactly simultaneous.

Nevertheless, I will cut out the 9 days of **zero** calorie expenditure, since that is physically very unlikely and indicates an artifact. As well as days with 0 total Steps, which would indicate that someone just left their device at home. 
We discard duplicates, by checking there is only one record per date per Id. We filter out any record with fewer than 15 SedentaryMinutes. It is hard to think of someone who had no rest at all during the day. Conversely, I get rid of all days with more than (1440-15) sedentary minutes, (1440 is the total number of minutes in a day).
Our final sanity check for this data set is to sum the total number of minutes of all activity types, and check that it is not greater than 1440, which is the total number of minutes in a day.

```{r filter daily zero cal}
#filter zero calorie expenditure
dailyActivity <- dailyActivity %>% filter(Calories > 1, TotalSteps > 1)

#filter less than 15 sedentary minutes 
dailyActivity <- dailyActivity %>% filter(between(SedentaryMinutes, 15,1435))

#Check that there are no Id, Date duplicate records
get_dupes(dailyActivity, Id, ActivityDate)

#Check that the sum of minutes does not exceed the total minutes in a day
dailyActivity %>% mutate(totalMinutes = VeryActiveMinutes + FairlyActiveMinutes + LightlyActiveMinutes + SedentaryMinutes) %>% filter(totalMinutes > 24 * 60)
```



#### Sleep data
Sleep data is very irregular and it is not clear why. There is also a tendency for the users with very few days to have too little or too much sleep. I will not filter out these users because there is also information of how often they use the device for tracking their sleep, so it is useful. It has to be kept in mind when analyzing the sleep durations, though. The number of minutes sleeping in general seems plausible.

```{r plotting sleep data, echo=FALSE}
ggplot(sleepDay, aes(x = SleepDay, y = Id)) + geom_tile(aes(fill = TotalMinutesAsleep)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Daily Sleep", subtitle = "Daily minutes spent asleep by user")
```

The sanity checks are no record Id/SleepDay duplication and that minutes in a day do not exceed $60 * 24 = 1440$, although we have already seen this is not the case in the skim_without_charts summary. 
```{r clean sleep}
get_dupes(sleepDay, Id, SleepDay)
##There are duplicate rows
sleepDay <- distinct(sleepDay)

sleepDay %>% filter(TotalMinutesAsleep >= 1440, TotalTimeInBed >= 1440)

sleepDay <- sleepDay %>% group_by(Id) %>% filter(n()>3)
```


#### Checking the date time in the hourly data

Checking for duplicates of Id/hour combination. Interestingly, there are no hours with zero calories, which is perplexing given that I thought the daily activity file was just a summary from this one.

```{r hourly check, warning=FALSE}
hourlyActivity %>% get_dupes(Id, ActivityHour)
#zero calory hours
filter(hourlyActivity, Calories == 0)
# days with more than 24 hours. Not really needed, only possible if get_dupes had returned something
hourlyActivity %>% mutate(Day = date(ActivityHour)) %>% 
    group_by(Id, Day) %>%
    summarise(number_of_hours = n()) %>% 
    filter(number_of_hours > 24)
```

We will not graph all the hours, instead we will group by day.

```{r echo=FALSE, message=FALSE, warning=FALSE}
hourly2dailyActivity <- hourlyActivity %>% mutate(Day = date(ActivityHour)) %>% group_by(Id, Day) %>%
    summarise(number_of_hours = n(), DCalories = sum(Calories), DStep = sum(StepTotal))

hourly2dailyActivity %>% ggplot(aes(x = Day, y = Id)) + geom_tile(aes(fill = DCalories)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Daily Calorie expenditure", subtitle = "Hourly calorie expenditure by user summarised by date")
```

This is really baffling. There is Calories data for almost all users for the first half of the study, the data is collected for 24 hours except on the last days. This data is missing in the DailyActivity (compare with the previous graphs). This might be due to the lack of Distance data for the relevant dates. Nevertheless, it could be useful to use this aggregated data if we only want to look at Calories or steps. Lastly, we might consider getting rid of Id "2891001357" with too few data points.

Now we look at the mean intensity throught the 24 hour window.

```{r}
hourlyActivity %>% mutate(hour_of_day = hour(ActivityHour)) %>% group_by(Id,hour_of_day) %>% summarise(MeanIntensity = mean(TotalIntensity)) %>% ggplot(aes(x = hour_of_day, y = Id)) + geom_tile(aes(fill = MeanIntensity)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Mean Hourly Intensity", subtitle = "Mean intensity by hour of the day")
```

This makes me more secure to filter our the Id "2891001357" that has very few days and those days are incomplete.

```{r filter incomplete user}
dailyActivity <- dailyActivity %>%  filter(Id != "2891001357")
hourly2dailyActivity <- hourly2dailyActivity %>%  filter(Id != "2891001357")
hourlyActivity <- hourlyActivity %>%  filter(Id != "2891001357")
minuteActivity <- minuteActivity  %>%  filter(Id != "2891001357")
```

Next, we will compare the hour aggregated data set with the existing dailyActivity data frame. We use a left join to keep only the dates that exist in the original dailyActivity.

```{r, echo=FALSE}
left_join(dailyActivity, hourly2dailyActivity, by = join_by(Id, ActivityDate == Day)) %>% transmute(Id, ActivityDate, diffCal = Calories - DCalories, diffSteps = TotalSteps - DStep, number_of_hours) %>% ggplot(aes(x = ActivityDate, y = Id)) + geom_tile(aes(fill = diffCal)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Daily Calorie error", subtitle = "Difference between daily and hourly aggregated data per user per day")
```

Most days coincide, some error could be expected due to aggregation issues or rounding. But what about the substantial divergences? The biggest discrepancies occur in days that are not on the edges and that have the full 24 hours. One of these days is also the maximum calories in the activity data. The other is an instance in which the number of sedentary minutes is 1440 (The whole day), yet the distance and number of steps are huge. I do not see how this is possible, only if the person kept the tracker in the backpack all day or something, therefore, I will erase these two records that differ the most in steps or calories between the daily and the hourly data.

```{r echo=FALSE}
left_join(dailyActivity, hourly2dailyActivity, by = join_by(Id, ActivityDate == Day)) %>% mutate(diffCal = Calories - DCalories, diffSteps = TotalSteps - DStep) %>% filter(abs(diffCal) == max(abs(diffCal), na.rm = T))

left_join(dailyActivity, hourly2dailyActivity, by = join_by(Id, ActivityDate == Day)) %>% mutate(diffCal = Calories - DCalories, diffSteps = TotalSteps - DStep) %>% filter(abs(diffSteps) == max(abs(diffSteps), na.rm = T))

dailyActivity <- dailyActivity %>%  filter(!(Id == "8583815059" & ActivityDate == ymd(20160503)))

dailyActivity <- dailyActivity %>%  filter(!(Id == "6117666160" & ActivityDate == ymd(20160421)))
```

### Daily activity revisited

This motivates me to explore the relationships in the dailyActivity data to understand it better. It seems from the graph that The "TotalDistance" is calculated from the number of steps for each individual user. That is the reason that TotalDistance is different from tracked distance, which comes from GPS.

```{r steps and distance, echo = F}
dailyActivity %>% ggplot() + geom_point(aes(x = TotalSteps, y = TotalDistance)) + theme_bw() + labs(title = "The Total distance is calculated from Steps", subtitle = "Daily Total steps have a linear relationship with distance")
```

We can sum the active distances from different excercise intensities (Active, Moderate, Light and Sedentary) and check their relationship with the total distance.

```{r}
dailyActivity %>% mutate(ActiveDistances = VeryActiveDistance + ModeratelyActiveDistance + LightActiveDistance + SedentaryActiveDistance)  %>%  ggplot() + geom_point(aes(x = ActiveDistances, y = TotalDistance)) + theme_bw() + labs(title = "Sum of active distances vs Total Distance", subtitle = "The distances roughly coincide")
```

We see that the graph is completely linear and that the sum of the distances is the same as the total distance in most cases. The total distance is never less than the sum of Active Distances, showing that the error is not random. I do not trust the points that diverge by more than 1 km. Specially the ones where the Active Distance is zero. An explanation for this is that people carried their device that day on their backpack or something, therefore steps or GPS were recorded, but not the heartrate, which determines intensity. Therefore, we will filter them out:

```{r filter divergent distances}
dailyActivity <- dailyActivity %>% mutate(ActiveDistances = VeryActiveDistance + ModeratelyActiveDistance + LightActiveDistance + SedentaryActiveDistance, differencia = TotalDistance - ActiveDistances)  %>%  filter(differencia < 1) %>% select(-differencia)
```
I do not show this but this filter has the advantage of dropping more of the highly divergent data from the hourly data we saw above (see graph Daily Calorie Error), mostly for the user "402033...". This gives me more confidence that dropping these data is correct.

#### No distance without time

I want to do a further sanity check on the Active columns. I want to check that there are no instances where VeryActiveDistance of 0 corresponds to a non-zero VeryActiveMinutes, which would be inconsistent. For this, we can use the xor function. Xor() will return FALSE if the data is correct, that is if both values are non-zero, or if both are zero, any discrepancy will result in a TRUE value. Of course, it is possible to have 0 distance with positive minutes, by working out in a stationary bike, for example, so we further filter to get only the rows where the time is zero but the distance is non-zero.

```{r}
filter_vector <- xor(dailyActivity$VeryActiveDistance, dailyActivity$VeryActiveMinutes)
filter(dailyActivity, filter_vector, VeryActiveMinutes == 0)

filter_vector <- xor(dailyActivity$ ModeratelyActiveDistance, dailyActivity$ FairlyActiveMinutes)
filter(dailyActivity, filter_vector, FairlyActiveMinutes == 0)

filter_vector <- xor(dailyActivity$LightActiveDistance, dailyActivity$LightlyActiveMinutes)
filter(dailyActivity, filter_vector, LightlyActiveMinutes == 0)

filter_vector <- xor(dailyActivity$SedentaryActiveDistance, dailyActivity$SedentaryMinutes)
filter(dailyActivity, filter_vector, SedentaryMinutes == 0)
```

Our data is consistent in this way. There is no distance that is traveled in zero time, which would be impossible.

#### Cleaning minute data and comparisons with above levels

```{r minute2hour, message=FALSE, warning=FALSE}
minute2hour <- minuteActivity %>% mutate(ActivityHour = date(ActivityMinute) + hours(hour(ActivityMinute))) %>% select(-ActivityMinute) %>% group_by(Id, ActivityHour) %>% summarise(Calories = round(sum(Calories)), TotalIntensity = sum(Intensity), AverageIntensity = mean(Intensity), StepTotal = sum(Steps),  n_minutes = n())

minute2hour %>% filter(n_minutes != 60)

### Compare with hourly data
left_join(hourlyActivity, minute2hour, by = join_by(Id, ActivityHour)) %>% drop_na() %>%
    transmute(Id, ActivityHour, diffCal = Calories.x - Calories.y,
              diffAvgInt = AverageIntensity.x - AverageIntensity.y,
              diffStep = StepTotal.x - StepTotal.y, 
              diffTotInt = TotalIntensity.x - TotalIntensity.y) %>% filter(diffCal > 0 | diffAvgInt > 0 | diffStep > 0)

setdiff(select(hourlyActivity, Id, ActivityHour),
        select(minute2hour, Id, ActivityHour))
```

The aggregated minute data is basically identical to the hour data, except for 6 rows that are present in the hourly data. They are from the last day of the data range.

### Heart rate data cleaning

No duplicates

```{r}
get_dupes(heartrate_seconds)
```

```{r heart rate measurements by day, echo=FALSE, message=FALSE, warning=FALSE}
heartrate_seconds %>% mutate(heart_date = date(Time)) %>% group_by(Id, heart_date) %>% summarise(n_records = n()) %>% ggplot(aes(x = heart_date, y = Id)) + geom_tile(aes(fill = n_records)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Number of heart rate records", subtitle = "Number of heart rate records by day by user", x = "Date")
```

The heart rate records are very spotty. They also vary a lot in terms of the number per day, and even in the time resolution.

```{r hear rate hour, echo=FALSE, message=FALSE, warning=FALSE}
heartrate_seconds %>% mutate(heart_hour = hour(Time)) %>% group_by(Id, heart_hour) %>% summarise(mean_rate = mean(Value)) %>% ggplot(aes(x = heart_hour, y = Id)) + geom_tile(aes(fill = mean_rate)) + scale_fill_distiller(palette = "YlGnBu")  + theme_dark() + labs(title = "Mean hourly heart rate", subtitle = "Mean heart rate by user by hour of day", x = "Hour of day")
```

The heart rate data can be useful to explore the usage and exercise habits for a subset of users. I think this data is more useful in combination with other information. Therefore, we will aggregate this data frame into minute-level by taking the mean within a minute and keep it as such.

```{r message=FALSE, warning=FALSE}
#Take out the seconds info
second(heartrate_seconds$Time) <- 0

#Group and Summarise
heartrate_minutes <- heartrate_seconds %>% group_by(Id, Time) %>% summarise(BPM = mean(Value))

minuteAct_BPM <- inner_join(minuteActivity, heartrate_minutes, by = join_by(Id, ActivityMinute == Time))
##Join with minute activity, 
rm(heartrate_seconds)
```

Joining with the minute activity will lose a lot of the minute level data, so it cannot be used for all timespans but it should still be a good amount of data.


